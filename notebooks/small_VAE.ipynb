{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import *\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Bernoulli\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../scripts')\n",
    "from allDataLoaderVAE import VaeDataset\n",
    "#from plotting import make_vae_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_path = \"../data/head_gtex_gene_expression_norm_transposed.tsv\"\n",
    "dataset = VaeDataset(tsv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes=[0, 1, 4, 9]\n",
    "#\n",
    "#def stratified_sampler(labels):\n",
    "#    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "#    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "#    indices = torch.from_numpy(indices)\n",
    "#    return SubsetRandomSampler(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 18965])\n",
      "torch.Size([3, 18965])\n",
      "torch.Size([3, 18965])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    data_train = batch[\"data\"]\n",
    "    sampleid_train = batch[\"sample_id\"]\n",
    "    print(data_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparameterization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "\n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "\n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "\n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        return self.mu + self.sigma * self.sample_epsilon() # <- your code\n",
    "\n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return torch.distributions.normal.Normal(self.mu, self.sigma).log_prob(z) # <- your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VAE network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "\n",
    "\n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.observation_features, out_features=18965),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=18965, out_features=3600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=3600, out_features=1200),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=1200, out_features=2*latent_features) # <- note the 2*latent_features\n",
    "        )\n",
    "\n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=1200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1200, out_features=3600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=3600, out_features=18965),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=18965, out_features=self.observation_features)\n",
    "        )\n",
    "\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "\n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(z|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "\n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "\n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "\n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "\n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "\n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_logits = self.decoder(z)\n",
    "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output\n",
    "        return Bernoulli(logits=px_logits, validate_args=False)\n",
    "\n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "\n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "\n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "\n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "\n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "\n",
    "\n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "\n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "\n",
    "        # sample the prior\n",
    "        z = pz.rsample()\n",
    "\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "\n",
    "        return {'px': px, 'pz': pz, 'z': z}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variational inference class, using beta elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "\n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "\n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "\n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "\n",
    "        # compute the ELBO with and without the beta parameter:\n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl # <- your code here\n",
    "        beta_elbo = log_px - self.beta*kl # <- your code here\n",
    "\n",
    "        # loss # minus is there because we want to maximize and not minimize?\n",
    "        loss = -beta_elbo.mean()\n",
    "\n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl, 'log_qz': log_qz, 'log_pz' : log_pz, 'beta_elbo': beta_elbo}\n",
    "\n",
    "        return loss, diagnostics, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=18965, out_features=18965, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=18965, out_features=3600, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=3600, out_features=1200, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=1200, out_features=200, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1200, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1200, out_features=3600, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=3600, out_features=18965, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=18965, out_features=18965, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 100\n",
    "vae = VariationalAutoencoder(data_train[0].size(), latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 0.1\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "\n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "\n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "\n",
    "        # Just load a single batch from the test loader\n",
    "        x, y = next(iter(test_loader))\n",
    "        x = x.to(device)\n",
    "\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "\n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "\n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    make_vae_plots(vae, x, y, outputs, training_data, validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BioAlgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
